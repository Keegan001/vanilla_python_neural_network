{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNljEi3iRBzTn662/c9IPM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keegan001/vanilla_python_neural_network/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S5tHN9dqGrL",
        "outputId": "50ad70a3-6f14-4f75-ce30-927380f6e87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14000/14000 [01:24<00:00, 166.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy 97.88 %\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy.special import logsumexp\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "class MLP():\n",
        "\n",
        "    def __init__(self, din, dout):\n",
        "        self.W = (2 * np.random.rand(dout, din) - 1) * (np.sqrt(6) / np.sqrt(din + dout))\n",
        "        self.b = (2 * np.random.rand(dout) - 1) * (np.sqrt(6) / np.sqrt(din + dout))\n",
        "\n",
        "    def forward(self, x): # x.shape = (batch_size, din)\n",
        "        self.x = x # Storing x for latter (backward pass)\n",
        "        return x @ self.W.T + self.b\n",
        "\n",
        "    def backward(self, gradout):\n",
        "        self.deltaW = gradout.T @ self.x\n",
        "        self.deltab = gradout.sum(0)\n",
        "        return gradout @ self.W\n",
        "\n",
        "class SequentialNN():\n",
        "\n",
        "    def __init__(self, blocks: list):\n",
        "        self.blocks = blocks\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backward(self, gradout):\n",
        "\n",
        "        for block in self.blocks[::-1]:\n",
        "            gradout = block.backward(gradout)\n",
        "\n",
        "        return gradout\n",
        "\n",
        "class ReLU():\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, gradout):\n",
        "        new_grad = gradout.copy()\n",
        "        new_grad[self.x < 0] = 0.\n",
        "        return new_grad\n",
        "\n",
        "class LogSoftmax():\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x - logsumexp(x, axis=1)[..., None]\n",
        "\n",
        "    def backward(self, gradout):\n",
        "        gradients = np.eye(self.x.shape[1])[None, ...]\n",
        "        gradients = gradients - (np.exp(self.x) / np.sum(np.exp(self.x), axis=1)[..., None])[..., None]\n",
        "        return (np.matmul(gradients, gradout[..., None]))[:, :, 0]\n",
        "\n",
        "class NLLLoss():\n",
        "\n",
        "    def forward(self, pred, true):\n",
        "        self.pred = pred\n",
        "        self.true = true\n",
        "\n",
        "        loss = 0\n",
        "        for b in range(pred.shape[0]):\n",
        "            loss -= pred[b, true[b]]\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        din = self.pred.shape[1]\n",
        "        jacobian = np.zeros((self.pred.shape[0], din))\n",
        "        for b in range(self.pred.shape[0]):\n",
        "            jacobian[b, self.true[b]] = -1\n",
        "\n",
        "        return jacobian # batch_size x din\n",
        "\n",
        "    def __call__(self, pred, true):\n",
        "        return self.forward(pred, true)\n",
        "\n",
        "class Optimizer():\n",
        "\n",
        "    def __init__(self, lr, compound_nn: SequentialNN):\n",
        "        self.lr = lr\n",
        "        self.compound_nn = compound_nn\n",
        "\n",
        "    def step(self):\n",
        "\n",
        "        for block in self.compound_nn.blocks:\n",
        "            if block.__class__ == MLP:\n",
        "                block.W = block.W - self.lr * block.deltaW\n",
        "                block.b = block.b - self.lr * block.deltab\n",
        "\n",
        "def train(model, optimizer, trainX, trainy, loss_fct = NLLLoss(), nb_epochs=14000, batch_size=100):\n",
        "    training_loss = []\n",
        "    for epoch in tqdm(range(nb_epochs)):\n",
        "\n",
        "        # Sample batch size\n",
        "        batch_idx = [np.random.randint(0, trainX.shape[0]) for _ in range(batch_size)]\n",
        "        x = trainX[batch_idx]\n",
        "        target = trainy[batch_idx]\n",
        "\n",
        "        prediction = model.forward(x) # Forward pass\n",
        "        loss_value = loss_fct(prediction, target) # Compute the loss\n",
        "        training_loss.append(loss_value) # Log loss\n",
        "        gradout = loss_fct.backward()\n",
        "        model.backward(gradout) # Backward pass\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "    return training_loss\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and process data\n",
        "    (trainX, trainy), (testX, testy) = load_data()\n",
        "    trainX = (trainX - 127.5) / 127.5\n",
        "    testX = (testX - 127.5) / 127.5\n",
        "    trainX = trainX.reshape(trainX.shape[0], 28 * 28)\n",
        "\n",
        "    mlp = SequentialNN([MLP(28*28, 128), ReLU(),\n",
        "                        MLP(128, 64), ReLU(),\n",
        "                        MLP(64, 10), LogSoftmax()])\n",
        "    optimizer = Optimizer(1e-3, mlp)\n",
        "\n",
        "    training_loss = train(mlp, optimizer, trainX, trainy)\n",
        "\n",
        "    # Compute test accuracy\n",
        "    accuracy = 0\n",
        "    for i in range(testX.shape[0]):\n",
        "        prediction = mlp.forward(testX[i].reshape(1, 784)).argmax()\n",
        "        if prediction == testy[i]: accuracy += 1\n",
        "    print('Test accuracy', accuracy / testX.shape[0] * 100, '%')"
      ]
    }
  ]
}